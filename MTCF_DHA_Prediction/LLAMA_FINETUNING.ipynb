{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9cc99ed-b4fa-4a0d-903a-145e7f561f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import bitsandbytes as bnb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "from datasets import Dataset\n",
    "from peft import LoraConfig, PeftConfig\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from trl import setup_chat_format\n",
    "from transformers import (AutoModelForCausalLM, \n",
    "                          AutoTokenizer, \n",
    "                          BitsAndBytesConfig, \n",
    "                          TrainingArguments, \n",
    "                          pipeline, \n",
    "                          logging)\n",
    "from sklearn.metrics import (accuracy_score, \n",
    "                             classification_report, \n",
    "                             confusion_matrix)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d2af27-532d-4b8b-b982-0f76de659462",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "# Load the JSON file (assumed to be a list of entries)\n",
    "with open(\"crash_narratives.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "\n",
    "# Shuffle the data to ensure randomness\n",
    "random.shuffle(data)\n",
    "\n",
    "# Define split ratios (70% train, 15% evaluation, 15% test)\n",
    "n = len(data)\n",
    "train_size = int(0.7 * n)\n",
    "eval_size = int(0.15 * n)\n",
    "# The rest of the data will be the test set\n",
    "test_size = n - train_size - eval_size\n",
    "\n",
    "# Split the data\n",
    "train_data = data[:train_size]\n",
    "eval_data = data[train_size:train_size + eval_size]\n",
    "test_data = data[train_size + eval_size:]\n",
    "\n",
    "# Save split datasets to separate JSONL files\n",
    "def save_jsonl(dataset, filename):\n",
    "    with open(filename, \"w\") as f:\n",
    "        for entry in dataset:\n",
    "            # Write each entry as a JSON-formatted string followed by a newline\n",
    "            f.write(json.dumps(entry) + \"\\n\")\n",
    "\n",
    "save_jsonl(train_data, \"train_data.jsonl\")\n",
    "save_jsonl(eval_data, \"eval_data.jsonl\")\n",
    "save_jsonl(test_data, \"test_data.jsonl\")\n",
    "\n",
    "# Print dataset sizes\n",
    "print(f\"Total samples: {n}\")\n",
    "print(f\"Training data: {len(train_data)} samples\")\n",
    "print(f\"Evaluation data: {len(eval_data)} samples\")\n",
    "print(f\"Test data: {len(test_data)} samples\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad54265d-4395-44f1-9650-7bc2faa33f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_jsonl_to_json(jsonl_file, json_file):\n",
    "    with open(jsonl_file, \"r\") as f:\n",
    "        data = [json.loads(line) for line in f]\n",
    "    with open(json_file, \"w\") as f:\n",
    "        json.dump(data, f)\n",
    "\n",
    "convert_jsonl_to_json(\"train_data.jsonl\", \"train_data.json\")\n",
    "convert_jsonl_to_json(\"eval_data.jsonl\", \"eval_data.json\")\n",
    "convert_jsonl_to_json(\"test_data.jsonl\", \"test_data.json\")\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "data_files = {\n",
    "    \"train\": \"train_data.json\",\n",
    "    \"validation\": \"eval_data.json\",\n",
    "    \"test\": \"test_data.json\"\n",
    "}\n",
    "\n",
    "dataset = load_dataset(\"json\", data_files=data_files)\n",
    "\n",
    "print(dataset['train'][0])\n",
    "print(dataset['validation'][0])\n",
    "print(dataset['test'][0])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd60ebd-2a31-43b9-a391-4042f853a799",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=dataset['train']\n",
    "eval_data =dataset['validation']\n",
    "test_data=dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35590368-9654-42ad-b4d1-f7f5353c8e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_name = \"meta-llama/Llama-3.2-1B\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=\"float16\",\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=\"float16\",\n",
    "    quantization_config=bnb_config, \n",
    ")\n",
    "\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "tokenizer.padding_side = \"left\"\n",
    "special_tokens = [\"<Speed and Stopping Violations>\", \n",
    "                  \"<Right-of-Way and Traffic Control Violations>\", \"<Lane and Direction Violations>\", \n",
    "                  \"<Maneuvering and Signaling Errors>\", \n",
    "                  \"<General Unsafe Driving>\", \"<NO HAZARDOUS ACTION>\", \"<BOTH DRIVERS TOOK HAZARDOUS ACTION>\"]\n",
    "tokenizer.add_special_tokens({\"additional_special_tokens\": special_tokens})\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "model.resize_token_embeddings(len(tokenizer), mean_resizing=False)\n",
    "\n",
    "\n",
    "import bitsandbytes as bnb\n",
    "\n",
    "def find_all_linear_names(model):\n",
    "    cls = bnb.nn.Linear4bit\n",
    "    lora_module_names = set()\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, cls):\n",
    "            names = name.split('.')\n",
    "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "    if 'lm_head' in lora_module_names:  # needed for 16 bit\n",
    "        lora_module_names.remove('lm_head')\n",
    "    return list(lora_module_names)\n",
    "modules = find_all_linear_names(model)\n",
    "\n",
    "\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "class CustomDataCollator(DataCollatorForLanguageModeling):\n",
    "    def __call__(self, examples):\n",
    "        batch = super().__call__(examples)\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        labels = batch[\"labels\"]\n",
    "        for i in range(len(input_ids)):\n",
    "            # Find assistant header\n",
    "            assistant_idx = (input_ids[i] == tokenizer.convert_tokens_to_ids(\"<|start_header_id|>\")).nonzero(as_tuple=True)[0]\n",
    "            assistant_end_idx = (input_ids[i] == tokenizer.convert_tokens_to_ids(\"<|end_header_id|>\")).nonzero(as_tuple=True)[0]\n",
    "            # Get the <|end_header_id|> after assistant\n",
    "            for start, end in zip(assistant_idx, assistant_end_idx):\n",
    "                if tokenizer.decode(input_ids[i][start:end+1]) == \"<|start_header_id|>assistant<|end_header_id|>\":\n",
    "                    assistant_end_pos = end\n",
    "                    break\n",
    "            # Skip the newline (\\n) after <|end_header_id|>\n",
    "            target_pos = assistant_end_pos + 2  # +1 for \\n, +2 for the target token\n",
    "            # Verify the target token\n",
    "            target_token = tokenizer.decode([input_ids[i][target_pos]])\n",
    "            # print(f\"Sample {i}: Target position = {target_pos}, Token = {target_token}\")\n",
    "            # Mask labels: -100 everywhere except the target position\n",
    "            mask = torch.ones_like(labels[i]) * -100\n",
    "            mask[target_pos] = labels[i][target_pos]\n",
    "            labels[i] = mask\n",
    "        batch[\"labels\"] = labels\n",
    "        return batch\n",
    "\n",
    "output_dir=\"MTCFLLM-llama-3.2-fine-tuned-model\"\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=64,\n",
    "    lora_dropout=0.05,\n",
    "    r=128,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=modules,\n",
    ")\n",
    "\n",
    "training_arguments = SFTConfig(\n",
    "    output_dir=output_dir,                    # directory to save and repository id\n",
    "    num_train_epochs=2,                       # number of training epochs\n",
    "    per_device_train_batch_size=32,            # batch size per device during training\n",
    "    gradient_accumulation_steps=16,            # number of steps before performing a backward/update pass\n",
    "    gradient_checkpointing=True,              # use gradient checkpointing to save memory\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    logging_steps=5,                         \n",
    "    learning_rate=1e-4,                       # learning rate, based on QLoRA paper\n",
    "    weight_decay=0.001,\n",
    "    fp16=False,\n",
    "    bf16=True,\n",
    "    packing=False,\n",
    "    max_seq_length=512,\n",
    "    dataset_text_field='text',\n",
    "    dataset_kwargs={\n",
    "    \"add_special_tokens\": True,\n",
    "    \"append_concat_token\": False,\n",
    "    },\n",
    "    max_grad_norm=0.3,                        # max gradient norm based on QLoRA paper\n",
    "    max_steps=-1,\n",
    "    warmup_ratio=0.02,                        # warmup ratio based on QLoRA paper\n",
    "    group_by_length=False,\n",
    "    lr_scheduler_type=\"cosine\",               # use cosine learning rate scheduler\n",
    "    report_to=\"wandb\",                  # report metrics to w&b\n",
    "    eval_strategy=\"steps\",              # save checkpoint every epoch\n",
    "    eval_steps = 200,\n",
    "    save_strategy=\"no\"\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_arguments,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=eval_data,\n",
    "    peft_config=peft_config,\n",
    "    processing_class=tokenizer,\n",
    "    data_collator=CustomDataCollator(tokenizer=tokenizer, mlm=False)\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "trainer.train()\n",
    "# 1) Save the fineâ€‘tuned LoRA adapters + base config\n",
    "trainer.save_model(output_dir)  \n",
    "\n",
    "# 2) Save the tokenizer (with new special tokens)\n",
    "tokenizer.save_pretrained(output_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
