{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67c5720-b219-436a-9b90-95d3c471eb87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    "from peft import PeftModel\n",
    "\n",
    "model_name_or_path = \"MTCFLLM-llama-3.2-fine-tuned-model\"  # saved both tokenizer + adapter\n",
    "base_model_id       = \"meta-llama/Llama-3.2-1B\"     # the *original* upstream\n",
    "\n",
    "# 1) load tokenizer (it already has the 7 new tokens)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "\n",
    "# 2) load the *base* LLaMA, in bfloat16\n",
    "base = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# 3) resize\n",
    "base.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# 4) load adapter weights on top\n",
    "model = PeftModel.from_pretrained(\n",
    "    base,\n",
    "    model_name_or_path,\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# after model.eval()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model.to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6fb5b7a-57ef-4e8e-b7e9-97db8755bd63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── SPECIAL TOKENS SETUP ────────────────────────────────────────────────────────\n",
    "special_tokens = [\n",
    "    \"<Speed and Stopping Violations>\",\n",
    "    \"<Right-of-Way and Traffic Control Violations>\",\n",
    "    \"<Lane and Direction Violations>\",\n",
    "    \"<Maneuvering and Signaling Errors>\",\n",
    "    \"<General Unsafe Driving>\",\n",
    "    \"<NO HAZARDOUS ACTION>\",\n",
    "    \"<BOTH DRIVERS TOOK HAZARDOUS ACTION>\"\n",
    "]\n",
    "special_token_ids = {\n",
    "    token: tokenizer.convert_tokens_to_ids(token)\n",
    "    for token in special_tokens\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# ─── HELPERS ─────────────────────────────────────────────────────────────────────\n",
    "def extract_prompt(text: str) -> str:\n",
    "    \"\"\"Return everything up to and including the assistant marker.\"\"\"\n",
    "    marker = \"<|start_header_id|>assistant<|end_header_id|>\\n\"\n",
    "    idx = text.find(marker)\n",
    "    return text[: idx + len(marker)] if idx != -1 else text\n",
    "\n",
    "def extract_ground_truth(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Grab the line immediately after the assistant marker and before <|eot_id|>.\n",
    "    \"\"\"\n",
    "    pattern = r\"<\\|start_header_id\\|>assistant<\\|end_header_id\\|>\\n(.*?)\\n<\\|eot_id\\|>\"\n",
    "    match = re.search(pattern, text, re.DOTALL)\n",
    "    return match.group(1).strip() if match else None\n",
    "\n",
    "# ─── EVALUATION LOOP ────────────────────────────────────────────────────────────\n",
    "predicted_tokens = []\n",
    "ground_truth_tokens = []\n",
    "\n",
    "for sample in test_data:\n",
    "    text = sample[\"text\"]\n",
    "    true_token = extract_ground_truth(text)\n",
    "    if true_token is None:\n",
    "        continue\n",
    "\n",
    "    prompt = extract_prompt(text)\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "    attention_mask = torch.ones_like(input_ids, dtype=torch.long, device=device)\n",
    "\n",
    "    # Use AMP autocast to ensure activations are in bfloat16\n",
    "    with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits[:, -1, :]                 # shape (1, vocab_size)\n",
    "        probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "\n",
    "        # pick the special token with highest probability\n",
    "        special_probs = {\n",
    "            tok: probs[0, tok_id].item()\n",
    "            for tok, tok_id in special_token_ids.items()\n",
    "        }\n",
    "        pred = max(special_probs, key=special_probs.get)\n",
    "\n",
    "    predicted_tokens.append(pred)\n",
    "    ground_truth_tokens.append(true_token)\n",
    "\n",
    "# ─── METRICS & PLOTTING ─────────────────────────────────────────────────────────\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(\n",
    "    ground_truth_tokens, predicted_tokens,\n",
    "    labels=special_tokens\n",
    ")\n",
    "disp = ConfusionMatrixDisplay(\n",
    "    confusion_matrix=cm,\n",
    "    display_labels=special_tokens\n",
    ")\n",
    "plt.figure(figsize=(10, 8))\n",
    "disp.plot(cmap=plt.cm.Blues, xticks_rotation=45, ax=plt.gca())\n",
    "plt.title(\"Confusion Matrix for Hazardous Action Prediction\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Classification report\n",
    "print(\"Classification Report:\\n\")\n",
    "print(classification_report(\n",
    "    ground_truth_tokens, predicted_tokens,\n",
    "    labels=special_tokens, zero_division=0\n",
    "))\n",
    "\n",
    "# Create a mapping from token to its ID in the tokenizer vocabulary\n",
    "special_token_ids = {token: tokenizer.convert_tokens_to_ids(token) for token in special_tokens}\n",
    "\n",
    "def extract_prompt(text):\n",
    "    \"\"\"\n",
    "    Extracts the prompt from the test sample text.\n",
    "    The prompt is everything up to and including the assistant marker.\n",
    "    \"\"\"\n",
    "    marker = \"<|start_header_id|>assistant<|end_header_id|>\\n\"\n",
    "    idx = text.find(marker)\n",
    "    if idx != -1:\n",
    "        # Return text including the marker so the model knows to generate right after it\n",
    "        return text[: idx + len(marker)]\n",
    "    else:\n",
    "        return text\n",
    "\n",
    "def extract_ground_truth(text):\n",
    "    \"\"\"\n",
    "    Extracts the ground truth token from the test sample text.\n",
    "    It is assumed to be the token following the assistant marker and before the next <|eot_id|>.\n",
    "    \"\"\"\n",
    "    pattern = r\"<\\|start_header_id\\|>assistant<\\|end_header_id\\|>\\n(.*?)\\n<\\|eot_id\\|>\"\n",
    "    match = re.search(pattern, text, re.DOTALL)\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Dictionary to accumulate probability lists for each token.\n",
    "probabilities = {token: [] for token in special_tokens}\n",
    "\n",
    "model.eval()\n",
    "# Loop over the test data samples (assuming test_data is a list of dicts with key 'text')\n",
    "for sample in test_data:\n",
    "    text = sample['text']\n",
    "    \n",
    "    # Extract the input prompt from the text.\n",
    "    prompt = extract_prompt(text)\n",
    "    \n",
    "    # Tokenize the prompt and move to GPU.\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt').to('cuda')\n",
    "    attention_mask = torch.ones_like(input_ids, dtype=torch.long).to('cuda')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits[:, -1, :]  # logits for the next token\n",
    "        probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "        \n",
    "        # For each special token, store its probability\n",
    "        for token in special_tokens:\n",
    "            token_id = special_token_ids[token]\n",
    "            probabilities[token].append(probs[0, token_id].item())\n",
    "\n",
    "# Prepare data for box plot: a list of probability lists in the order of special_tokens\n",
    "data_to_plot = [probabilities[token] for token in special_tokens]\n",
    "\n",
    "# Create a box plot to show the probability distribution for each token\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.boxplot(data_to_plot, labels=special_tokens, showfliers=False)\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.ylabel(\"Probability\")\n",
    "plt.title(\"Probability Distribution for Each Predicted Token\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# If each sub‑list is of equal length, you can convert to a 2D array:\n",
    "arr = np.array(data_to_plot)\n",
    "np.save(\"data_to_plot.npy\", arr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
